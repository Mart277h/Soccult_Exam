---
title: "Model validation"
author: "Martine Lind Jensen"
        "Sarah Hvid Andersen"
date: "26/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Loading packages
pacman::p_load(tidyverse, lme4, Metrics, viridis)

#Reading data 
mv_df <- read_csv("data_csv/abm_verification_1st.csv") %>% rename(Knowledge_Tick = Knowledge_percent)

stats <- read_csv("data_csv/folkeskolen_stats.csv")

#Reading data from second validation csv
mv_df_second <- read_csv("data_csv/abm_verification_2nd.csv")
```

Checking baseline attention for paper
```{r}
#Plotting baseline attention values 
mv_df %>% ggplot(aes(x = Baseline.attention)) +
  geom_histogram(bins = 100) + 
  theme_classic() + 
  ggtitle("Baseline attention distribution") + 
  xlab("Baseline Attention")+ 
  ylab("Count")
```

##Creating grades 

As our model spans from a max attained knowledge to a min attained knowledge, this would be the same as min = -3 max = 12. Though, the Danish grade 12 does not mean the maximum amount you can learn, therefore, we put
```{r}
#Maximum attained knowledge = 12 we set to 10200(as you can be better than 12, but you cant be worse than -3) It is also more than 2 sd away from the mean (can be argumented to be outliers)
max_k <- 10200 #max(mv_df$End_knowledge)

    #mean(mv_df_grade$End_knowledge) + 2 * sd(mv_df_grade$End_knowledge)

#Minimum attained knowledge = -3, we set to 2000, to keep the same scale in all (more than 3sd away - the minimum amount of knowledge)
min_k <- 2000 #min(mv_df$End_knowledge)

    #(mean(mv_df_grade$End_knowledge) - 3 * sd(mv_df_grade$End_knowledge)) - 1322.5

#Finding difference
diff <- max_k-min_k

#Difference divided by 15 should give us what 0.01 increment in 7-step grade should be
inc <- diff/1500

#Making a grade for each student based on their end_knowledge 
  #If larger than 12, give 12
mv_df_grade <- mv_df %>% dplyr::rowwise() %>% 
  mutate(Grade = round((((End_knowledge-min_k)/inc)-300)/100, 2)) %>% mutate(Grade = ifelse(Grade > 12, 12, Grade)) %>% mutate(Grade = ifelse(Grade < -3, -3, Grade))
```

Plotting data 
```{r}
#Plotting 
mv_df_grade %>% ggplot(aes(x = Grade)) + 
  geom_histogram(bins = 30)+
  stat_bin(bins= 30, geom="text", aes(label=round((..count..)/sum(..count..)*100, 2))) +
  scale_y_continuous(labels = scales::percent_format()) + 
  ggtitle("Grade distribution from first model validation")
    #Looks good 

mv_df_grade %>% ggplot(aes(x= Grade)) +
  geom_density()
```

##Model validation
Stats first try 
```{r}
#Run some summaries 
summary(mv_df_grade$Grade)

#Dividing into the same distribution as folkeskolen stats
under2 <- mv_df_grade %>% filter(Grade < 2) %>% nrow() / nrow(mv_df_grade) *100
twofour <- mv_df_grade %>% filter(Grade >= 2 & Grade < 4) %>% nrow() / nrow(mv_df_grade)*100
fourseven <- mv_df_grade%>% filter(Grade >= 4 & Grade < 7) %>% nrow() / nrow(mv_df_grade)*100
seventen <- mv_df_grade %>% filter(Grade >= 7 & Grade < 10) %>% nrow() / nrow(mv_df_grade)*100
overten <- mv_df_grade %>% filter(Grade >= 10 ) %>% nrow() / nrow(mv_df_grade)*100

df_12 <- mv_df_grade %>% filter(Grade == 12) %>% nrow() / nrow(mv_df_grade)*100
  #Under 1 percent gets clean 12
```

##Second model validation
Stats second try 
```{r}
#Creating
mv_df_second <- mv_df_second %>% dplyr::rowwise() %>% 
  mutate(Grade = round((((End_knowledge-min_k)/inc)-300)/100, 2)) %>% mutate(Grade = ifelse(Grade > 12, 12, Grade)) %>% mutate(Grade = ifelse(Grade < -3, -3, Grade))

#Making stats 
under2_2 <- mv_df_second %>% filter(Grade < 2) %>% nrow() / nrow(mv_df_second) *100
twofour_2 <- mv_df_second %>% filter(Grade >= 2 & Grade < 4) %>% nrow() / nrow(mv_df_second)*100
fourseven_2 <- mv_df_second%>% filter(Grade >= 4 & Grade < 7) %>% nrow() / nrow(mv_df_second)*100
seventen_2 <- mv_df_second %>% filter(Grade >= 7 & Grade < 10) %>% nrow() / nrow(mv_df_second)*100
overten_2 <- mv_df_second %>% filter(Grade >= 10 ) %>% nrow() / nrow(mv_df_second)*100
```

Plotting new data 
```{r}
#Plotting 
mv_df_second %>% ggplot(aes(x = Grade)) + 
  stat_bin(bins = 30)+
  stat_bin(bins= 30, geom="text", aes(label=round((..count..)/sum(..count..)*100, 2))) +
  scale_y_continuous(labels = scales::percent_format()) + 
  ggtitle("Grade distribution from second model validation")
    #Looks good 

mv_df_second %>% ggplot(aes(x= Grade)) +
  geom_density()
```

#Creating stats dataframe
Comparing the different models 
```{r}
#Index 
Grades <- c("Under 1.99", "2.00-3.99", "4.00-6.99", "7.00-9.99", "Over 10.00")
#First stats 
mv_first <- rbind(under2, twofour, fourseven, seventen, overten)

#Second stats
mv_second <- rbind(under2_2, twofour_2, fourseven_2, seventen_2, overten_2)

#Folkeskole stats 
folkeskolen_mean <- rbind(1.3, 9.29, 32.14, 41.29, 15.98) 
folkeskolen_sd <- rbind(0.3, 0.89, 1.77, 1.57, 1.30)

#Making dataframe 
stats_mv <- data.frame(Grades, folkeskolen_mean, folkeskolen_sd, mv_first, mv_second)
    #Looks so fine 

#Error calculations 
stats_mv <- stats_mv %>% dplyr::rowwise() %>%  
  mutate(Error_first = round((folkeskolen_mean - mv_first)/folkeskolen_mean*100, 2)) %>% 
  mutate(Error_second = round((folkeskolen_mean - mv_second)/folkeskolen_mean*100, 2)) %>% 
  mutate(mv_first = round(mv_first, 2), 
         mv_second = round(mv_second, 2)) 

#Sum of error between the two validations
sum(stats_mv$Error_first)
sum(stats_mv$Error_second)

#Root mean squared error 
rmse(folkeskolen_mean, mv_first)
rmse(folkeskolen_mean, mv_second)

#Models 
#m1 <- lm(folkeskolen_mean^2 ~ mv_first^2, stats_mv)
m2 <- lm(folkeskolen_mean ~ mv_second, stats_mv)
#plot(m2)

#Looks like model 2 is better (higher multiple r s-squared) explaining very much of the variance.
#summary(m1) 
summary(m2)
  
#Model 2 explains most variance == better model 
```

```{r}
#Plotting the two models against each other 

ggplot() +
  geom_density(aes(x = Grade, fill = "First model validation"), data = mv_df_grade, alpha = 0.7) + 
  geom_density(aes( x= Grade, fill = "Second model validation"), data = mv_df_second, alpha = 0.7) + 
  ggtitle("First model validation against second model validation") + 
  scale_fill_manual(name="Legend", values=c("coral1", "cadetblue3")) + 
  theme(legend.position = c(0.8, 0.2)) +
  theme_classic() + 
  ylab("Density")

```


##Creating grades from knowledge pr tick

Calculating grades from knowledge pr tick instead. Follow same procedure. And comparing between grades, This is probably not necessary for the hypothesis.

```{r}
#Maximum attained knowledge = 12 we set to 10200(as you can be better than 12, but you cant be worse than -3) It is also more than 2 sd away from the mean (can be argumented to be outliers)
max_kt <- 0.8878 #max(mv_df$Knowledge_Tick)

    #mean(mv_df_grade$Knowledge_Tick) + 2 * sd(mv_df_grade$Knowledge_Tick)

#Minimum attained knowledge = -3, we set to 2000, to keep the same scale in all (more than 3sd away - the minimum amount of knowledge)
min_kt <- 0.1748042 #min(mv_df$Knowledge_Tick)

    #(mean(mv_df_grade$End_knowledge) + 3 * sd(mv_df_grade$End_knowledge)) - 1322.5

#Finding difference
diffkt <- max_kt-min_kt

#Difference divided by 1500 should give us what 0.01 increment in 7-step grade should be
inckt <- diffkt/1500

#Making a grade for each student based on their end_knowledge 
  #If larger than 12, give 12
mv_df_grade <- mv_df_grade %>% dplyr::rowwise() %>% 
  mutate(Grade_kt = round((((Knowledge_Tick-min_kt)/inckt)-300)/100, 2)) %>% mutate(Grade_kt = ifelse(Grade_kt > 12, 12, Grade_kt)) %>% mutate(Grade_kt = ifelse(Grade_kt < -3, -3, Grade_kt))

#Are grade_kt different from grade? 
mv_df_grade$diff <- mv_df_grade$Grade-mv_df_grade$Grade_kt

#So many percent are different from 0 
diff_percent <- mv_df_grade %>% filter(diff != 0) %>% nrow() / nrow(mv_df_grade)*100

#Mean and sd of the two kinds of grades 
grades_compare <- data.frame(mean(mv_df_grade$Grade_kt), mean(mv_df_grade$Grade), sd(mv_df_grade$Grade_kt), sd(mv_df_grade$Grade)) 

#Is it significantly different from zero? 
t.test(mv_df_grade$diff)
  #Yes though with a very small mean which we deem okay for further analysis in hypothesis 2
```



